{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usmanzafar\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2717: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "X = pd.read_csv('C:\\\\Users\\\\usmanzafar\\\\Desktop\\\\refinedperm.csv')\n",
    "# limit to categorical data using df.select_dtypes()\n",
    "X = X.select_dtypes(include=[object])\n",
    "X['pw_amount_9089'] = X['pw_amount_9089'].str.replace(',', '')\n",
    "X=X.dropna(how='any') \n",
    "X.to_csv('C:\\\\Users\\\\usmanzafar\\\\Downloads\\\\refinedperm.csv',index=False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94122, 16)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv('C:\\\\Users\\\\usmanzafar\\\\Downloads\\\\refinedperm.csv')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Certified            40995\n",
       "Certified-Expired    40002\n",
       "Denied                7130\n",
       "Withdrawn             5995\n",
       "Name: case_status, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.case_status.unique()\n",
    "X.case_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: create a LabelEncoder object and fit it to each feature in X\n",
    "# 1. INSTANTIATE\n",
    "# encode labels with value between 0 and n_classes-1.\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "# 2/3. FIT AND TRANSFORM\n",
    "# use df.apply() to apply le.fit_transform to all columns\n",
    "X_2 = X.apply(le.fit_transform)\n",
    "X_2.head()\n",
    "X_2.reset_index(drop=True)\n",
    "X_2.case_status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical classes: [0 1 2 3]\n",
      "Integer classes: [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "enc = LabelEncoder()\n",
    "label_encoder = enc.fit(X_2['case_status'])\n",
    "print (\"Categorical classes:\", label_encoder.classes_)\n",
    "integer_classes = label_encoder.transform(label_encoder.classes_)\n",
    "print (\"Integer classes:\", integer_classes)\n",
    "x_train = label_encoder.transform(X_2['case_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " ..., \n",
      " [0]\n",
      " [3]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train[:,np.newaxis]\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1  17 118 ..., 566   3   4]\n",
      " [  1  17 130 ...,   1   3   4]\n",
      " [  1  28 136 ..., 563   3   4]\n",
      " ..., \n",
      " [  0  17 560 ..., 505   3   4]\n",
      " [  3  17 546 ..., 127   3   4]\n",
      " [  0  17 562 ..., 118   3   4]]\n"
     ]
    }
   ],
   "source": [
    "X_2.reset_index(drop=True)\n",
    "features=np.asmatrix(X_2)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ..., 0 3 0]\n"
     ]
    }
   ],
   "source": [
    "x_train.shape\n",
    "b= x_train.reshape(94122,)\n",
    "b.shape\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usmanzafar\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************8\n",
      "classification report for random forest classifier! with under sampling \n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.10      1.00      0.96      0.18      0.31      0.09         6\n",
      "          1       0.71      0.80      1.00      0.75      0.84      0.68        15\n",
      "          2       0.13      0.75      0.95      0.22      0.36      0.12        16\n",
      "          3       1.00      0.91      1.00      0.95      0.46      0.23      1493\n",
      "\n",
      "avg / total       0.98      0.91      1.00      0.94      0.47      0.23      1530\n",
      "\n",
      "\n",
      "******************************************************************\n",
      "classification report for Logistic Regression! with undersampling\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.56      0.83      1.00      0.67      0.75      0.53         6\n",
      "          1       0.11      0.53      0.96      0.18      0.33      0.10        15\n",
      "          2       0.01      0.69      0.50      0.03      0.12      0.01        16\n",
      "          3       0.99      0.45      0.78      0.62      0.18      0.04      1493\n",
      "\n",
      "avg / total       0.97      0.46      0.78      0.61      0.19      0.04      1530\n",
      "\n",
      "\n",
      "******************************************************************\n",
      "classification report for Naive Bayes using Gaussian function! with undersampling\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       1.00      1.00      1.00      1.00      1.00      1.00         6\n",
      "          1       0.37      1.00      0.98      0.54      0.60      0.34        15\n",
      "          2       0.01      0.94      0.12      0.02      0.11      0.01        16\n",
      "          3       1.00      0.09      1.00      0.16      0.16      0.03      1493\n",
      "\n",
      "avg / total       0.98      0.11      0.99      0.17      0.17      0.04      1530\n",
      "\n",
      "\n",
      "******************************************************************\n",
      "classification report for Decision tree! with undersampling\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       1.00      1.00      1.00      1.00      1.00      1.00         6\n",
      "          1       1.00      1.00      1.00      1.00      1.00      1.00        15\n",
      "          2       1.00      1.00      1.00      1.00      1.00      1.00        16\n",
      "          3       1.00      1.00      1.00      1.00      1.00      1.00      1493\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1.00      1.00      1.00      1530\n",
      "\n",
      "\n",
      "classification report for Gradient boosting classifier!  with undersampling\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       1.00      1.00      1.00      1.00      1.00      1.00         6\n",
      "          1       1.00      1.00      1.00      1.00      1.00      1.00        15\n",
      "          2       1.00      1.00      1.00      1.00      1.00      1.00        16\n",
      "          3       1.00      1.00      1.00      1.00      1.00      1.00      1493\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1.00      1.00      1.00      1530\n",
      "\n",
      "\n",
      "******************************************************************\n",
      "classification report for MLP neural network! with undersampling\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.00      0.17      0.56      0.00      0.04      0.00         6\n",
      "          1       0.00      0.00      0.97      0.00      0.00      0.00        15\n",
      "          2       0.00      0.00      1.00      0.00      0.00      0.00        16\n",
      "          3       0.98      0.53      0.49      0.69      0.16      0.03      1493\n",
      "\n",
      "avg / total       0.95      0.52      0.50      0.67      0.15      0.03      1530\n",
      "\n",
      "\n",
      "******************************************************************\n",
      "classification report for K nearest neighbor classifier with undersampling\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.01      0.50      0.63      0.01      0.07      0.00         6\n",
      "          1       0.01      0.33      0.69      0.02      0.10      0.01        15\n",
      "          2       0.01      0.19      0.81      0.02      0.10      0.01        16\n",
      "          3       0.96      0.12      0.78      0.22      0.14      0.02      1493\n",
      "\n",
      "avg / total       0.94      0.13      0.78      0.21      0.14      0.02      1530\n",
      "\n",
      "******************************************************************\n",
      "\n",
      "classification report for Linear Support Vector Machine with undersampling\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.00      0.00      1.00      0.00      0.00      0.00         6\n",
      "          1       0.00      0.00      1.00      0.00      0.00      0.00        15\n",
      "          2       0.01      1.00      0.00      0.02      0.10      0.01        16\n",
      "          3       1.00      0.00      1.00      0.01      0.16      0.03      1493\n",
      "\n",
      "avg / total       0.98      0.01      0.99      0.01      0.15      0.03      1530\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usmanzafar\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\usmanzafar\\Anaconda3\\lib\\site-packages\\imblearn\\metrics\\classification.py:244: UndefinedMetricWarning: Sensitivity is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.datasets import make_imbalance\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### code for the under sampling \n",
    "\n",
    "RANDOM_STATE = 42\n",
    "X1, y1 = make_imbalance(features, b, ratio={0: 25, 1: 50, 2: 50},\n",
    "                      random_state=0)\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(\n",
    "    X1, y1, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "# Create a pipeline\n",
    "# Every pipeline is for a different classifier\n",
    "pipeline1 = make_pipeline(NearMiss(version=2, random_state=RANDOM_STATE),\n",
    "                         RandomForestClassifier(random_state=RANDOM_STATE))\n",
    "pipeline2 = make_pipeline(NearMiss(version=2, random_state=RANDOM_STATE),\n",
    "                         LogisticRegression(random_state=RANDOM_STATE))\n",
    "pipeline3 = make_pipeline(NearMiss(version=2, random_state=RANDOM_STATE),\n",
    "                         GaussianNB())\n",
    "pipeline4 = make_pipeline(NearMiss(version=2, random_state=RANDOM_STATE),\n",
    "                        tree.DecisionTreeClassifier(random_state=RANDOM_STATE))\n",
    "pipeline5 = make_pipeline(NearMiss(version=2, random_state=RANDOM_STATE),\n",
    "                         GradientBoostingClassifier(random_state=RANDOM_STATE,n_estimators= 18))\n",
    "pipeline6 = make_pipeline(NearMiss(version=2, random_state=RANDOM_STATE),\n",
    "                         MLPClassifier(alpha = 1,random_state=RANDOM_STATE))\n",
    "pipeline7 = make_pipeline(NearMiss(version=2, random_state=RANDOM_STATE),\n",
    "                         KNeighborsClassifier())\n",
    "pipeline8 = make_pipeline(NearMiss(version=2, random_state=RANDOM_STATE),\n",
    "                         SVC(random_state=RANDOM_STATE))\n",
    "\n",
    "# Fitting the data for diffferent pipelines for different classifiers \n",
    "pipeline1.fit(X1_train, y1_train)\n",
    "pipeline2.fit(X1_train, y1_train)\n",
    "pipeline3.fit(X1_train, y1_train)\n",
    "pipeline4.fit(X1_train, y1_train)\n",
    "pipeline5.fit(X1_train, y1_train)\n",
    "pipeline6.fit(X1_train, y1_train)\n",
    "pipeline7.fit(X1_train, y1_train)\n",
    "pipeline8.fit(X1_train, y1_train)\n",
    "\n",
    "# Classify and report the results, for every differnent classifier\n",
    "\n",
    "##printing the results fro random forest classifier \n",
    "print(\"******************************************************************8\")\n",
    "print(\"classification report for random forest classifier! with under sampling \")\n",
    "print(classification_report_imbalanced(y1_test, pipeline1.predict(X1_test)))\n",
    "print(\"\")\n",
    "\n",
    "## printing results for logistic regression\n",
    "print (\"******************************************************************\")\n",
    "print(\"classification report for Logistic Regression! with undersampling\")\n",
    "print(classification_report_imbalanced(y1_test, pipeline2.predict(X1_test)))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "print (\"******************************************************************\")\n",
    "print(\"classification report for Naive Bayes using Gaussian function! with undersampling\")\n",
    "print(classification_report_imbalanced(y1_test, pipeline3.predict(X1_test)))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "print (\"******************************************************************\")\n",
    "print(\"classification report for Decision tree! with undersampling\")\n",
    "print(classification_report_imbalanced(y1_test, pipeline4.predict(X1_test)))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"classification report for Gradient boosting classifier!  with undersampling\")\n",
    "print(classification_report_imbalanced(y1_test, pipeline5.predict(X1_test)))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "print (\"******************************************************************\")\n",
    "print(\"classification report for MLP neural network! with undersampling\")\n",
    "print(classification_report_imbalanced(y1_test, pipeline6.predict(X1_test)))\n",
    "print(\"\")\n",
    "\n",
    "print (\"******************************************************************\")\n",
    "print(\"classification report for K nearest neighbor classifier with undersampling\")\n",
    "print(classification_report_imbalanced(y1_test, pipeline7.predict(X1_test)))\n",
    "print (\"******************************************************************\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"classification report for Linear Support Vector Machine with undersampling\")\n",
    "print(classification_report_imbalanced(y1_test, pipeline8.predict(X1_test)))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
